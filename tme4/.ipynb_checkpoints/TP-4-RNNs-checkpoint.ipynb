{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 4 Réseaux récurrents \n",
    "\n",
    "Réalisé par: \n",
    " - Yacine ALLAOUACHE \n",
    " - Kenza ELMAM \n",
    "\n",
    "Notable TODOs:\n",
    "- Implemenetation d'un RNN \n",
    "- Classification de séquences \n",
    "- Modéle de séquences multi-variées\n",
    "- Génération de séquences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "## Library import\n",
    "We import all the required Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Options for pandas\n",
    "pd.options.display.max_columns = 50\n",
    "pd.options.display.max_rows = 30\n",
    "\n",
    "# Visualizations\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as ply\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "\n",
    "import cufflinks as cf\n",
    "cf.go_offline(connected=True)\n",
    "cf.set_config_file(theme='white')\n",
    "\n",
    "import matplotlib as plt\n",
    "\n",
    "# Autoreload extension\n",
    "if 'autoreload' not in get_ipython().extension_manager.loaded:\n",
    "    %load_ext autoreload\n",
    "    \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local library import\n",
    "We import all the required local libraries libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Include local library paths\n",
    "import sys\n",
    "sys.path.append('./src') # uncomment and fill to import local libraries\n",
    "\n",
    "from  utils import * \n",
    "import exo2 \n",
    "import exo3\n",
    "import exo4\n",
    "\n",
    "# Import local libraries\n",
    "%load_ext autoreload\n",
    "    \n",
    "%autoreload 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Data import\n",
    "We retrieve all the required data for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{\"./data/trump_full_speech.txt\"}') as f:\n",
    "    trump_full_speech = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>Vancouver</th>\n",
       "      <th>Portland</th>\n",
       "      <th>San Francisco</th>\n",
       "      <th>Seattle</th>\n",
       "      <th>Los Angeles</th>\n",
       "      <th>San Diego</th>\n",
       "      <th>Las Vegas</th>\n",
       "      <th>Phoenix</th>\n",
       "      <th>Albuquerque</th>\n",
       "      <th>Denver</th>\n",
       "      <th>San Antonio</th>\n",
       "      <th>Dallas</th>\n",
       "      <th>Houston</th>\n",
       "      <th>Kansas City</th>\n",
       "      <th>Minneapolis</th>\n",
       "      <th>Saint Louis</th>\n",
       "      <th>Chicago</th>\n",
       "      <th>Nashville</th>\n",
       "      <th>Indianapolis</th>\n",
       "      <th>Atlanta</th>\n",
       "      <th>Detroit</th>\n",
       "      <th>Jacksonville</th>\n",
       "      <th>Charlotte</th>\n",
       "      <th>Miami</th>\n",
       "      <th>Pittsburgh</th>\n",
       "      <th>Toronto</th>\n",
       "      <th>Philadelphia</th>\n",
       "      <th>New York</th>\n",
       "      <th>Montreal</th>\n",
       "      <th>Boston</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-10-01 13:00:00</td>\n",
       "      <td>284.63</td>\n",
       "      <td>282.08</td>\n",
       "      <td>289.48</td>\n",
       "      <td>281.8</td>\n",
       "      <td>291.87</td>\n",
       "      <td>291.53</td>\n",
       "      <td>293.41</td>\n",
       "      <td>296.6</td>\n",
       "      <td>285.12</td>\n",
       "      <td>284.61</td>\n",
       "      <td>289.29</td>\n",
       "      <td>289.74</td>\n",
       "      <td>288.27</td>\n",
       "      <td>289.98</td>\n",
       "      <td>286.87</td>\n",
       "      <td>286.18</td>\n",
       "      <td>284.01</td>\n",
       "      <td>287.41</td>\n",
       "      <td>283.85</td>\n",
       "      <td>294.03</td>\n",
       "      <td>284.03</td>\n",
       "      <td>298.17</td>\n",
       "      <td>288.65</td>\n",
       "      <td>299.72</td>\n",
       "      <td>281.0</td>\n",
       "      <td>286.26</td>\n",
       "      <td>285.63</td>\n",
       "      <td>288.22</td>\n",
       "      <td>285.83</td>\n",
       "      <td>287.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-10-01 14:00:00</td>\n",
       "      <td>284.62904131</td>\n",
       "      <td>282.083251974</td>\n",
       "      <td>289.474992813</td>\n",
       "      <td>281.797216632</td>\n",
       "      <td>291.86818552200003</td>\n",
       "      <td>291.533500952</td>\n",
       "      <td>293.403141271</td>\n",
       "      <td>296.60850854299997</td>\n",
       "      <td>285.154558187</td>\n",
       "      <td>284.607305531</td>\n",
       "      <td>289.303648787</td>\n",
       "      <td>289.762974207</td>\n",
       "      <td>288.297575758</td>\n",
       "      <td>289.997635363</td>\n",
       "      <td>286.893635588</td>\n",
       "      <td>286.18524602900004</td>\n",
       "      <td>284.05469097400004</td>\n",
       "      <td>287.42136028</td>\n",
       "      <td>283.889393939</td>\n",
       "      <td>294.03534141</td>\n",
       "      <td>284.06978923400004</td>\n",
       "      <td>298.205229759</td>\n",
       "      <td>288.650172214</td>\n",
       "      <td>299.732517698</td>\n",
       "      <td>281.024767377</td>\n",
       "      <td>286.262540958</td>\n",
       "      <td>285.663207797</td>\n",
       "      <td>288.24767617</td>\n",
       "      <td>285.834649953</td>\n",
       "      <td>287.186092094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-10-01 15:00:00</td>\n",
       "      <td>284.626997923</td>\n",
       "      <td>282.091866475</td>\n",
       "      <td>289.460618112</td>\n",
       "      <td>281.789832606</td>\n",
       "      <td>291.86284445900003</td>\n",
       "      <td>291.54335507900004</td>\n",
       "      <td>293.392177052</td>\n",
       "      <td>296.631487354</td>\n",
       "      <td>285.233951595</td>\n",
       "      <td>284.5999178</td>\n",
       "      <td>289.338496754</td>\n",
       "      <td>289.830766948</td>\n",
       "      <td>288.334343434</td>\n",
       "      <td>290.038150556</td>\n",
       "      <td>286.951400772</td>\n",
       "      <td>286.199194111</td>\n",
       "      <td>284.177412183</td>\n",
       "      <td>287.454636935</td>\n",
       "      <td>283.941919192</td>\n",
       "      <td>294.049702185</td>\n",
       "      <td>284.173964682</td>\n",
       "      <td>298.299595186</td>\n",
       "      <td>288.650581705</td>\n",
       "      <td>299.76657946</td>\n",
       "      <td>281.08831873599996</td>\n",
       "      <td>286.269518418</td>\n",
       "      <td>285.756824139</td>\n",
       "      <td>288.326939663</td>\n",
       "      <td>285.847789539</td>\n",
       "      <td>287.23167159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-10-01 16:00:00</td>\n",
       "      <td>284.624954535</td>\n",
       "      <td>282.100480976</td>\n",
       "      <td>289.446243412</td>\n",
       "      <td>281.78244858</td>\n",
       "      <td>291.857503395</td>\n",
       "      <td>291.55320920599996</td>\n",
       "      <td>293.381212832</td>\n",
       "      <td>296.65446616400004</td>\n",
       "      <td>285.31334500400004</td>\n",
       "      <td>284.59253007</td>\n",
       "      <td>289.373344722</td>\n",
       "      <td>289.89855969</td>\n",
       "      <td>288.371111111</td>\n",
       "      <td>290.07866575</td>\n",
       "      <td>287.009165955</td>\n",
       "      <td>286.213142193</td>\n",
       "      <td>284.300133393</td>\n",
       "      <td>287.48791359</td>\n",
       "      <td>283.994444444</td>\n",
       "      <td>294.064062959</td>\n",
       "      <td>284.278140131</td>\n",
       "      <td>298.393960613</td>\n",
       "      <td>288.650991196</td>\n",
       "      <td>299.800641223</td>\n",
       "      <td>281.151870096</td>\n",
       "      <td>286.27649587900004</td>\n",
       "      <td>285.85044048</td>\n",
       "      <td>288.406203155</td>\n",
       "      <td>285.860929124</td>\n",
       "      <td>287.277251086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-10-01 17:00:00</td>\n",
       "      <td>284.622911147</td>\n",
       "      <td>282.109095477</td>\n",
       "      <td>289.431868711</td>\n",
       "      <td>281.775064553</td>\n",
       "      <td>291.85216233200003</td>\n",
       "      <td>291.563063333</td>\n",
       "      <td>293.370248613</td>\n",
       "      <td>296.677444975</td>\n",
       "      <td>285.392738413</td>\n",
       "      <td>284.585142339</td>\n",
       "      <td>289.40819268900003</td>\n",
       "      <td>289.96635243099996</td>\n",
       "      <td>288.407878788</td>\n",
       "      <td>290.119180943</td>\n",
       "      <td>287.066931139</td>\n",
       "      <td>286.227090275</td>\n",
       "      <td>284.42285460200003</td>\n",
       "      <td>287.521190245</td>\n",
       "      <td>284.046969697</td>\n",
       "      <td>294.078423734</td>\n",
       "      <td>284.38231558</td>\n",
       "      <td>298.488326039</td>\n",
       "      <td>288.65140068700003</td>\n",
       "      <td>299.83470298599997</td>\n",
       "      <td>281.215421455</td>\n",
       "      <td>286.283473339</td>\n",
       "      <td>285.944056822</td>\n",
       "      <td>288.485466648</td>\n",
       "      <td>285.874068709</td>\n",
       "      <td>287.322830583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              datetime      Vancouver       Portland  San Francisco  \\\n",
       "0  2012-10-01 13:00:00         284.63         282.08         289.48   \n",
       "1  2012-10-01 14:00:00   284.62904131  282.083251974  289.474992813   \n",
       "2  2012-10-01 15:00:00  284.626997923  282.091866475  289.460618112   \n",
       "3  2012-10-01 16:00:00  284.624954535  282.100480976  289.446243412   \n",
       "4  2012-10-01 17:00:00  284.622911147  282.109095477  289.431868711   \n",
       "\n",
       "         Seattle         Los Angeles           San Diego      Las Vegas  \\\n",
       "0          281.8              291.87              291.53         293.41   \n",
       "1  281.797216632  291.86818552200003       291.533500952  293.403141271   \n",
       "2  281.789832606  291.86284445900003  291.54335507900004  293.392177052   \n",
       "3   281.78244858       291.857503395  291.55320920599996  293.381212832   \n",
       "4  281.775064553  291.85216233200003       291.563063333  293.370248613   \n",
       "\n",
       "              Phoenix         Albuquerque         Denver         San Antonio  \\\n",
       "0               296.6              285.12         284.61              289.29   \n",
       "1  296.60850854299997       285.154558187  284.607305531       289.303648787   \n",
       "2       296.631487354       285.233951595    284.5999178       289.338496754   \n",
       "3  296.65446616400004  285.31334500400004   284.59253007       289.373344722   \n",
       "4       296.677444975       285.392738413  284.585142339  289.40819268900003   \n",
       "\n",
       "               Dallas        Houston    Kansas City    Minneapolis  \\\n",
       "0              289.74         288.27         289.98         286.87   \n",
       "1       289.762974207  288.297575758  289.997635363  286.893635588   \n",
       "2       289.830766948  288.334343434  290.038150556  286.951400772   \n",
       "3        289.89855969  288.371111111   290.07866575  287.009165955   \n",
       "4  289.96635243099996  288.407878788  290.119180943  287.066931139   \n",
       "\n",
       "          Saint Louis             Chicago      Nashville   Indianapolis  \\\n",
       "0              286.18              284.01         287.41         283.85   \n",
       "1  286.18524602900004  284.05469097400004   287.42136028  283.889393939   \n",
       "2       286.199194111       284.177412183  287.454636935  283.941919192   \n",
       "3       286.213142193       284.300133393   287.48791359  283.994444444   \n",
       "4       286.227090275  284.42285460200003  287.521190245  284.046969697   \n",
       "\n",
       "         Atlanta             Detroit   Jacksonville           Charlotte  \\\n",
       "0         294.03              284.03         298.17              288.65   \n",
       "1   294.03534141  284.06978923400004  298.205229759       288.650172214   \n",
       "2  294.049702185       284.173964682  298.299595186       288.650581705   \n",
       "3  294.064062959       284.278140131  298.393960613       288.650991196   \n",
       "4  294.078423734        284.38231558  298.488326039  288.65140068700003   \n",
       "\n",
       "                Miami          Pittsburgh             Toronto   Philadelphia  \\\n",
       "0              299.72               281.0              286.26         285.63   \n",
       "1       299.732517698       281.024767377       286.262540958  285.663207797   \n",
       "2        299.76657946  281.08831873599996       286.269518418  285.756824139   \n",
       "3       299.800641223       281.151870096  286.27649587900004   285.85044048   \n",
       "4  299.83470298599997       281.215421455       286.283473339  285.944056822   \n",
       "\n",
       "        New York       Montreal         Boston  \n",
       "0         288.22         285.83         287.17  \n",
       "1   288.24767617  285.834649953  287.186092094  \n",
       "2  288.326939663  285.847789539   287.23167159  \n",
       "3  288.406203155  285.860929124  287.277251086  \n",
       "4  288.485466648  285.874068709  287.322830583  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperatures_train = pd.read_csv('./data/tempAMAL_train.csv', low_memory=False)\n",
    "temperatures_test  = pd.read_csv('./data/tempAMAL_test.csv', low_memory=False, header=None)\n",
    "temperatures_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette partie, on supprime les lignes vides dans train et test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "train = temperatures_train.iloc[:11115, 1:].dropna()\n",
    "train = pd.concat([train, temperatures_train.iloc[11116:-1, 1:].dropna()], axis=0)\n",
    "test = temperatures_test.iloc[:, 1:].dropna()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Classification des séquences "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour cette tache, on utilisera l'artichitecture many to one, On itere sur la longuer des séquences et on calcule un etat caché qui nous permettra de classer les sequences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/many_to_one.png\" style=\"display:block;margin-left:auto;margin-right:auto;width:50%;\"> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, on normalise les données en utilisant StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = StandardScaler().fit_transform(train).astype(np.float32)\n",
    "X_test = StandardScaler().fit_transform(test).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Définition des hyper-parametres \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'checkpoints/exo2/': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "rm -r checkpoints/exo2/\n",
    "rm -r experiments/exo2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_LABELS = 10\n",
    "latent_size = 200\n",
    "lr = 1e-3\n",
    "batch_size=16\n",
    "fixed_length_train = 10\n",
    "fixed_length_test = 10\n",
    "n_epochs = 100\n",
    "log_dir = \"./experiments/exo2\"\n",
    "checkpoint_dir = './checkpoints/exo2'\n",
    "checkpoint_path = f'{checkpoint_dir}/checkpoint_same_l_' + \\\n",
    "            exo2.datetime.now().strftime('%d_%m_%Y_%H:%M:%S')\n",
    "\n",
    "exo2.os.makedirs(f\"{log_dir}\", exist_ok=True)\n",
    "exo2.os.makedirs(f\"{checkpoint_dir}\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette partie, on separe les sequences (temperatures) et les labels (les villes)  afin de creer un dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_train_same, labels_train_same = exo2.training_examples(X_train, N_LABELS, length=fixed_length_train)\n",
    "sequences_test, labels_test =  exo2.training_examples(X_test, N_LABELS, length=fixed_length_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = SequencesDatasetWithSameLength(sequences_train_same,labels_train_same)\n",
    "data_test = SequencesDatasetWithSameLength(sequences_test,labels_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader_same = torch.utils.data.DataLoader(\n",
    "        data_train, batch_size=batch_size, shuffle=True)\n",
    "testloader_same = torch.utils.data.DataLoader(\n",
    "        data_test, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création du modele, loss et optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  exo2.Rnn_classifier(1,latent_size, N_LABELS).to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette partie, on entraine le modele sur nos données, on sauvegarde nos resultats accuracy et loss avec le tensorboard et checkpoints "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: Loss: 2.2361 Acc: 21.1%\tTest: Loss: 2.2834 Acc: 14.76%: 100%|██████████| 100/100 [02:04<00:00,  1.24s/it]\n"
     ]
    }
   ],
   "source": [
    "losses, accuracies =  exo2.train(\n",
    "        model=model,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        dataloader=trainloader_same,\n",
    "        test_loader=testloader_same,\n",
    "        n_epochs=n_epochs,\n",
    "        n_label=N_LABELS,\n",
    "        device=device,\n",
    "        log_dir=log_dir,\n",
    "        checkpoint_path=checkpoint_path\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On visualise nos resultats, on obtient une accuracy à 13.75% en train et 12% en test. On remarque que notre modele ne se converge pas malgré une exploration des hyper-parametres "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 36076), started 3:08:50 ago. (Use '!kill 36076' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-73669c0ffe4c10b1\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-73669c0ffe4c10b1\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./experiments/exo2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Modéles de séquences multi-variées "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pour cette tache, on utilisera l'artichitecture many to many, On itere sur la longuer de la séquence $T$ et on calcule un etat caché $h_{T}$,  ensuite on applique un decodage à l'instant $T+1$ et/ou $T+2$ pour predire la temperature à $T+1$ et/ou à $T+2$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/many-to-many.png\" style=\"display:block;margin-left:auto;margin-right:auto;width:50%;\"> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalisation des données \n",
    "\n",
    "On normalise nos données en utulisant MinMax afin que les valeurs soient entre 0 et 1 ce qui nous permets d'utiliser une sigmoid a la fin sortie de norte modele."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = exo3.MinMaxScaler().fit_transform(train).astype(np.float32)\n",
    "X_test = exo3.MinMaxScaler().fit_transform(test).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 RNN commun à toutes les villes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition des hyper-parametres \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_LABELS = 10\n",
    "latent_size = 20\n",
    "p_temps=2\n",
    "lr = 1e-4\n",
    "batch_size=16\n",
    "fixed_length_train = 27\n",
    "fixed_length_test = 27\n",
    "n_epochs = 100\n",
    "log_dir = \"./experiments/exo3/one_rnn\"\n",
    "checkpoint_dir = './checkpoints/exo3/one_rnn'\n",
    "checkpoint_path = f'{checkpoint_dir}/checkpoint_same_l_' + \\\n",
    "            exo3.datetime.now().strftime('%d_%m_%Y_%H:%M:%S')\n",
    "\n",
    "exo3.os.makedirs(f\"{log_dir}\", exist_ok=True)\n",
    "exo3.os.makedirs(f\"{checkpoint_dir}\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette partie, on separe les sequences (temperatures) et les labels (les villes)  afin de creer un dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_train_same, labels_train_same =  exo3.training_examples(X_train, N_LABELS, length=fixed_length_train,pas_de_temps=p_temps)\n",
    "sequences_test, labels_test = exo3.training_examples(X_test, N_LABELS, length=fixed_length_test,pas_de_temps=p_temps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = SequencesDatasetWithSameLength(sequences_train_same,labels_train_same)\n",
    "data_test = SequencesDatasetWithSameLength(sequences_test,labels_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader_same = torch.utils.data.DataLoader(\n",
    "        data_train, batch_size=batch_size, shuffle=True)\n",
    "testloader_same = torch.utils.data.DataLoader(\n",
    "        data_test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création du modele, loss et optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = exo3.Rnn_forecasting(    \n",
    "    input_size=1, \n",
    "    latent_size=latent_size, \n",
    "    output=N_LABELS, \n",
    "    pas_de_temp=p_temps\n",
    "    ).to(device)\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette partie, on entraine le modele sur nos données, on sauvegarde nos resultats accuracy et loss avec le tensorboard et checkpoints "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Loss: 0.0232\tTest: Loss: 0.0289: 100%|██████████| 100/100 [08:07<00:00,  4.88s/it]\n"
     ]
    }
   ],
   "source": [
    "losses = exo3.train(\n",
    "        model=model, \n",
    "        criterion=criterion, \n",
    "        optimizer=optimizer, \n",
    "        dataloader=trainloader_same, \n",
    "        test_loader=testloader_same, \n",
    "        n_epochs=n_epochs,\n",
    "        log_dir=log_dir,\n",
    "        checkpoint_path=checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 NN par série de température propre à une ville"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition des hyper-parametres \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "rm -r ./experiments/exo3/\n",
    "rm -r ./checkpoints/exo3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_villes = 10\n",
    "latent_size = 20\n",
    "p_temps=2\n",
    "lr = 1e-4\n",
    "batch_size=16\n",
    "fixed_length_train = 27\n",
    "fixed_length_test = 27\n",
    "n_epochs = 100\n",
    "log_dir = \"./experiments/exo3/multi_rnn\"\n",
    "checkpoint_dir = './checkpoints/exo3/multi_rnn'\n",
    "checkpoint_path = f'{checkpoint_dir}/checkpoint_same_l_' + \\\n",
    "            exo3.datetime.now().strftime('%d_%m_%Y_%H:%M:%S')\n",
    "\n",
    "exo3.os.makedirs(f\"{log_dir}\", exist_ok=True)\n",
    "exo3.os.makedirs(f\"{checkpoint_dir}\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette partie, on separe les sequences (temperatures) et les labels (les villes)  afin de creer un dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_train_same, labels_train_same =  exo3.training_examples(X_train, N_LABELS, length=fixed_length_train,pas_de_temps=p_temps)\n",
    "sequences_test, labels_test = exo3.training_examples(X_test, N_LABELS, length=fixed_length_test,pas_de_temps=p_temps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = SequencesDatasetWithSameLength(sequences_train_same,labels_train_same)\n",
    "data_test = SequencesDatasetWithSameLength(sequences_test,labels_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader_same = torch.utils.data.DataLoader(\n",
    "        data_train, batch_size=batch_size, shuffle=True)\n",
    "testloader_same = torch.utils.data.DataLoader(\n",
    "        data_test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création du modele, loss et optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = exo3.MultipleRNN(     \n",
    "    n_villes = n_villes,\n",
    "    input_size=1, \n",
    "    latent_size=latent_size, \n",
    "    pas_de_temp=p_temps\n",
    "    ).to(device)\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette partie, on entraine le modele sur nos données, on sauvegarde nos resultats accuracy et loss avec le tensorboard et checkpoints "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    }
   ],
   "source": [
    "losses = exo3.trainMR(\n",
    "        model=model, \n",
    "        criterion=criterion, \n",
    "        optimizer=optimizer, \n",
    "        dataloader=trainloader_same, \n",
    "        test_loader=testloader_same, \n",
    "        n_epochs=n_epochs,\n",
    "        log_dir=log_dir,\n",
    "        checkpoint_path=checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On visualise nos resultats, on obtient une loss à 0.002% en test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-9285cce3cb14b5f7\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-9285cce3cb14b5f7\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./experiments/exo3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Génération de séquences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L’objectif est de décoder à partir de l’état caché une distribution de probabilités multinomiale sur les symboles à engendrer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/one_to_many.png\" style=\"display:block;margin-left:auto;margin-right:auto;width:50%;\"> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition des hyper-parametres \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_size = 100\n",
    "lr = 1e-4\n",
    "length=50\n",
    "batch_size=64\n",
    "n_epochs = 10\n",
    "log_dir = \"./experiments/exo4\"\n",
    "checkpoint_dir = './checkpoints/exo4'\n",
    "checkpoint_path = f'{checkpoint_dir}/checkpoint_same_l_' + \\\n",
    "            exo4.datetime.now().strftime('%d_%m_%Y_%H:%M:%S')\n",
    "\n",
    "exo4.os.makedirs(f\"{log_dir}\", exist_ok=True)\n",
    "exo4.os.makedirs(f\"{checkpoint_dir}\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mise en forme des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = exo4.unidecode.unidecode(trump_full_speech)\n",
    "text = exo4.unicodedata.normalize('NFD', text)\n",
    "text = text.lower()\n",
    "text = text.translate(str.maketrans(\"\",\"\", exo4.re.sub('[\\.|,|;]', '', exo4.string.punctuation)))\n",
    "text = text.strip()\n",
    "\n",
    "LETTRES = set(text)\n",
    "N_LETTRES = len(LETTRES)+1\n",
    "\n",
    "id2lettre = dict(zip(range(1,len(LETTRES)+1),LETTRES))\n",
    "lettre2id = dict(zip(id2lettre.values(),id2lettre.keys()))\n",
    "\n",
    "X = exo4.string2code(text, lettre2id).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette partie, on cree un dataset de sequences ${\\{x_1,x_2,...,x_n\\}}$ du discours de Trump de taille variable, associée à une traget tel que $(x,y) = (x[:-1], x[1:])$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_train_same, labels_train_same =  exo4.training_examples(X, length, N_LETTRES)\n",
    "\n",
    "train_datset = SequencesDatasetWithSameLength(sequences_train_same,labels_train_same)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_datset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition notre model, optimizer, et la loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = exo4.Rnn_Generator(input_size=N_LETTRES, latent_size=latent_size, output=N_LETTRES).to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Loss: 3.6225: 100%|██████████| 10/10 [02:24<00:00, 14.40s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.662418092015278,\n",
       " 3.626484566424266,\n",
       " 3.6250544657190162,\n",
       " 3.6246031893305033,\n",
       " 3.6242653387138644,\n",
       " 3.6235087567065136,\n",
       " 3.623019082287708,\n",
       " 3.622653119535331,\n",
       " 3.6225445735885438,\n",
       " 3.62250502425504]"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exo4.train(\n",
    "        train=trainloader, \n",
    "        model=model, \n",
    "        criterion=criterion, \n",
    "        optimizer=optimizer, \n",
    "        n_lettres=N_LETTRES, \n",
    "        n_epochs=n_epochs, \n",
    "        log_dir=log_dir, \n",
    "        checkpoint_path=checkpoint_path\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6007 (pid 51807), started 0:04:51 ago. (Use '!kill 51807' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-caec72e1d96eaa00\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-caec72e1d96eaa00\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./experiments/exo4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'h pv3aoiy0mtigknwl6unkb7ijs,qceq7r8ob.pt6a\\nr\\nzcwz ;'"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exo4.generate_sequence(\n",
    "    model=model, \n",
    "    start=\"h\", \n",
    "    n_lettres=N_LETTRES, \n",
    "    length=50, \n",
    "    temperature=0.1, \n",
    "    lettre2id=lettre2id, \n",
    "    id2lettre=id2lettre\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('./implimentation-pl/')\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "from utils import LitModelClassifier, RNN, Temperature, nn, pl\n",
    "\n",
    "from utils import LitModelClassifier, RNN, Temperature, nn, pl, torch\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rnn_classifier(RNN):\n",
    "    def __init__(self, input_size, latent_size, output):\n",
    "        super(Rnn_classifier, self).__init__(input_size, latent_size)\n",
    "        self.out = nn.Linear(latent_size, output)\n",
    "        self.decision = nn.Softmax(dim=1)\n",
    "\n",
    "    def decode(self, h):\n",
    "        x = self.out(h)\n",
    "        return self.decision(x)\n",
    "    \n",
    "class Rnn_forecasting(RNN):\n",
    "    def __init__(self, input_size, latent_size, pas_de_temp):\n",
    "        super(Rnn_forecasting, self).__init__(input_size, latent_size)\n",
    "        self.out = nn.Linear(latent_size, 1)\n",
    "        self.decision = nn.Sigmoid()\n",
    "        self.pas_de_temp = pas_de_temp\n",
    "\n",
    "    def decode(self, last_h):\n",
    "        result = []\n",
    "        for _ in range(self.pas_de_temp):\n",
    "            x = self.out(last_h)\n",
    "            x = self.decision(x).squeeze(1)\n",
    "            result.append(x)\n",
    "\n",
    "        return torch.stack(result, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = Rnn_forecasting(1, 30, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = Temperature(\n",
    "                seq_length=15,\n",
    "                batch_size=32,\n",
    "                data_dir='./data/',\n",
    "                columns=list(range(5)),\n",
    "                task=2,\n",
    "                normalize=True\n",
    "            )\n",
    "litmodel=LitModelClassifier(backbone, learning_rate=1e-3, forcasting=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: None, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(progress_bar_refresh_rate=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | backbone  | Rnn_forecasting  | 1.0 K \n",
      "1 | criterion | CrossEntropyLoss | 0     \n",
      "2 | metric    | Accuracy         | 0     \n",
      "-----------------------------------------------\n",
      "1.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.0 K     Total params\n",
      "0.004     Total estimated model params size (MB)\n",
      "/home/melissa/anaconda3/envs/main/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80a14679438a4fd6ba526a8bf61894d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "1D target tensor expected, multi-target not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-a0cd2f855f02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlitmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatamodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/main/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloader, val_dataloaders, datamodule)\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0;31m# hook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'on_fit_start'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mteardown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/main/lib/python3.8/site-packages/pytorch_lightning/accelerators/legacy/cpu_accelerator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# train or test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_or_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/main/lib/python3.8/site-packages/pytorch_lightning/accelerators/legacy/accelerator.py\u001b[0m in \u001b[0;36mtrain_or_test\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/main/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_sanity_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;31m# set stage for logging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/main/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mrun_sanity_check\u001b[0;34m(self, ref_model)\u001b[0m\n\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             \u001b[0;31m# run eval step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_batches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_sanity_val_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m             \u001b[0;31m# allow no returns from eval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/main/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mrun_evaluation\u001b[0;34m(self, test_mode, max_batches)\u001b[0m\n\u001b[1;32m    644\u001b[0m                 \u001b[0;31m# lightning module methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"evaluation_step_and_end\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m                     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predicting\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m                         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/main/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py\u001b[0m in \u001b[0;36mevaluation_step\u001b[0;34m(self, test_mode, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mmodel_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_fx_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"validation_step\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"validation_step\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;31m# capture any logged information\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/main/lib/python3.8/site-packages/pytorch_lightning/accelerators/legacy/cpu_accelerator.py\u001b[0m in \u001b[0;36mvalidation_step\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/main/lib/python3.8/site-packages/pytorch_lightning/accelerators/legacy/cpu_accelerator.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, model_step, args)\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/yacine/m2/amal/Advanced-Machine-Learning-And-Deep-Learning/tme4/implimentation-pl/utils.py\u001b[0m in \u001b[0;36mvalidation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprog_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/main/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/main/lib/python3.8/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 961\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m    962\u001b[0m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/main/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2466\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2467\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2468\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/main/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2262\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[1;32m   2263\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2264\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2265\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2266\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: 1D target tensor expected, multi-target not supported"
     ]
    }
   ],
   "source": [
    "trainer.fit(litmodel, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n"
     ]
    }
   ],
   "source": [
    "for x, y in datamodule.train_dataloader():\n",
    "    out = backbone(x, backbone.initHidden(x.shape[0]))\n",
    "    logits = backbone.decode(out[-1])\n",
    "    print(logits.shape)\n",
    "    print(y.shape)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
