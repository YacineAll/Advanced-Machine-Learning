{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title\n",
    "The title of the notebook should be coherent with file name. Namely, file name should be:    \n",
    "*author's initials_progressive number_title.ipynb*    \n",
    "For example:    \n",
    "*EF_01_Data Exploration.ipynb*\n",
    "\n",
    "## Purpose\n",
    "State the purpose of the notebook.\n",
    "\n",
    "## Methodology\n",
    "Quickly describe assumptions and processing steps.\n",
    "\n",
    "## WIP - improvements\n",
    "Use this section only if the notebook is not final.\n",
    "\n",
    "Notable TODOs:\n",
    "- todo 1;\n",
    "- todo 2;\n",
    "- todo 3.\n",
    "\n",
    "## Results\n",
    "Describe and comment the most important results.\n",
    "\n",
    "## Suggested next steps\n",
    "State suggested next steps, based on results obtained in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "## Library import\n",
    "We import all the required Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Options for pandas\n",
    "pd.options.display.max_columns = 50\n",
    "pd.options.display.max_rows = 30\n",
    "\n",
    "# Visualizations\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as ply\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "\n",
    "import cufflinks as cf\n",
    "cf.go_offline(connected=True)\n",
    "cf.set_config_file(theme='white')\n",
    "\n",
    "import matplotlib as plt\n",
    "\n",
    "# Autoreload extension\n",
    "if 'autoreload' not in get_ipython().extension_manager.loaded:\n",
    "    %load_ext autoreload\n",
    "    \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local library import\n",
    "We import all the required local libraries libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Include local library paths\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "import sys\n",
    "sys.path.append('./src') # uncomment and fill to import local libraries\n",
    "\n",
    "from textloader import *\n",
    "from tp5 import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter definition\n",
    "We set all relevant parameters for our notebook. By convention, parameters are uppercase, while all the \n",
    "other variables follow Python's guidelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Data import\n",
    "We retrieve all the required data for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data/trump_full_speech.txt\"\n",
    "with open(f'{data_path}') as f:\n",
    "    text = f.readlines()\n",
    "text = text[0]+text[1]+text[2]\n",
    "text = re.sub('Trump: ','',text)\n",
    "text = re.sub('^Trump.$','',text)\n",
    "text = re.sub('\\[.*\\] ', '', text)\n",
    "text = text.strip()\n",
    "traindataset = TextDataset(text)\n",
    "trainloader = DataLoader(traindataset, batch_size=16, collate_fn=collate_fn, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(num_embeddings=len(id2lettre), embedding_dim=100, hidden_size=150, output_size=len(id2lettre))\n",
    "criterion = MaskedCrossEntropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, criterion, x, y, m):\n",
    "    x.unsqueeze_(-1)\n",
    "    h = [ t.to(device) for t in model.initHidden(x.shape[1])]\n",
    "    loss = 0\n",
    "    for i, x in enumerate(x):\n",
    "        h = model.one_step(x, *h)\n",
    "        logits = model.decode(h[0])\n",
    "        loss += criterion(logits, y[i], m[i])\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(iter(trainloader))\n",
    "n = 7\n",
    "x, y, m = x[:n], x[1:n+1], x[1:n+1]!=PAD_IX "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(num_embeddings=len(id2lettre), embedding_dim=100, hidden_size=150, output_size=len(id2lettre)).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = maskedCrossEntropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def train(train, model, criterion, optimizer, scheduler, n_epochs):\n",
    "    losses = []\n",
    "    pbar = tqdm(range(n_epochs), total=n_epochs, file=sys.stdout)\n",
    "    for _ in pbar:\n",
    "        l = []\n",
    "        for x in train:\n",
    "            x, y, m = x[:-1].to(device), x[1:].to(device), (x[1:]!=PAD_IX ).to(device)\n",
    "            \n",
    "            #logits, loss = train_step(model, criterion, x, y, m)\n",
    "            h = [ t.to(device) for t in model.initHidden(x.shape[1])] \n",
    "            d = model.decode(model(x, h))\n",
    "            loss = criterion(d, y, m)\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            l.append(loss.item())\n",
    "      \n",
    "        #scheduler.step()\n",
    "        \n",
    "        lo = np.mean(l)\n",
    "        losses.append(lo)\n",
    "        pbar.set_description(f'Train: Loss: {np.round(lo, 4)}') # \\tTest: Loss: {np.round(test_lo, 4)}\n",
    "        \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = train(trainloader, model, criterion, optimizer, None, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, start, maxlength):\n",
    "    x = torch.tensor(string2code(start)).unsqueeze(-1).to(device)\n",
    "    h = [ v.to(device) for v in model.initHidden(1)]\n",
    "    l = [lettre2id[start]]\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(maxlength):\n",
    "            h = model.one_step(x, *h)\n",
    "            d = model.decode(h[0])\n",
    "            predictions.append(d.squeeze(0).tolist())\n",
    "            probs = torch.exp(d)\n",
    "            start = torch.distributions.categorical.Categorical(probs).sample()\n",
    "            l.append(start.item())\n",
    "            if start.item() == EOS_IX:\n",
    "                break\n",
    "            start = start.unsqueeze(-1).to(device)\n",
    "    return code2string(l), predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "string, predictions = generate(model, \"H\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hh[4yC&g^/og-H?PgdV|r~zz<D2L~q~U3SK#>NW~G*s?|;c3|UHtK(cX9)5r-d#)ej)__<;qs%i'{/UxP/'Wb'Y_mK9cV-CK4^|\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = model.decode()\n",
    "d = model.decode(model(x, [ v.to(device) for v in model.initHidden(x.shape[1])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.4635, grad_fn=<MeanBackward0>)\n",
      "tensor(4.4298, grad_fn=<MeanBackward0>)\n",
      "tensor(4.4320, grad_fn=<MeanBackward0>)\n",
      "tensor(4.4291, grad_fn=<MeanBackward0>)\n",
      "tensor(4.4344, grad_fn=<MeanBackward0>)\n",
      "tensor(4.4259, grad_fn=<MeanBackward0>)\n",
      "tensor(4.4202, grad_fn=<MeanBackward0>)\n",
      "tensor(4.4348, grad_fn=<MeanBackward0>)\n",
      "tensor(4.4231, grad_fn=<MeanBackward0>)\n",
      "tensor(4.4267, grad_fn=<MeanBackward0>)\n",
      "tensor(4.4260, grad_fn=<MeanBackward0>)\n",
      "tensor(4.4160, grad_fn=<MeanBackward0>)\n",
      "tensor(4.4333, grad_fn=<MeanBackward0>)\n",
      "tensor(4.4234, grad_fn=<MeanBackward0>)\n",
      "tensor(4.4217, grad_fn=<MeanBackward0>)\n",
      "tensor(4.4324, grad_fn=<MeanBackward0>)\n",
      "tensor(4.4292, grad_fn=<MeanBackward0>)\n",
      "tensor(4.4232, grad_fn=<MeanBackward0>)\n",
      "tensor(4.4287, grad_fn=<MeanBackward0>)\n",
      "tensor(4.4237, grad_fn=<MeanBackward0>)\n",
      "tensor(4.4280, grad_fn=<MeanBackward0>)\n",
      "tensor(4.4224, grad_fn=<MeanBackward0>)\n",
      "tensor(4.4274, grad_fn=<MeanBackward0>)\n",
      "tensor(4.4325, grad_fn=<MeanBackward0>)\n",
      "tensor(4.4358, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for x in trainloader:\n",
    "    \n",
    "    x, y, m = x[:-1].to(device), x[1:].to(device), (x[1:]!=PAD_IX ).to(device)\n",
    "    #logits, loss = train_step(model, criterion, x, y, m)\n",
    "    h = [ t.to(device) for t in model.initHidden(x.shape[1])] \n",
    "    d = model.decode(model(x, h))\n",
    "    loss = criterion(d, y, m)\n",
    "    losses.append(loss.item())\n",
    "    print(loss)\n",
    "    #loss.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[96]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(string2code(start)).unsqueeze(-1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train_step(model, criterion, x, y, m):\n",
    "    x.unsqueeze_(-1)\n",
    "    h = model.initHidden(x.shape[1])\n",
    "    loss = 0\n",
    "    for i, x in enumerate(x):\n",
    "        h = model(x, *h)\n",
    "        logits = model.decode(h[0])\n",
    "        loss += criterion(logits, y[i], m[i])\n",
    "    return logits, loss\n",
    "\n",
    "\n",
    "def train(train, model, criterion, optimizer, scheduler, n_epochs):\n",
    "    losses = []\n",
    "    pbar = tqdm(range(n_epochs), total=n_epochs, file=sys.stdout)\n",
    "    for _ in pbar:\n",
    "        l = []\n",
    "        for x, mask in train:\n",
    "            x, y, m = x[:-1].to(device), x[1:].to(device), mask[1:].to(device)\n",
    "            \n",
    "            logits, loss = train_step(model, criterion, x, y, m)\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            l.append(loss.item()/len(x))\n",
    "      \n",
    "        #scheduler.step()\n",
    "        \n",
    "        lo = np.mean(l)\n",
    "        losses.append(lo)\n",
    "        pbar.set_description(f'Train: Loss: {np.round(lo, 4)}') # \\tTest: Loss: {np.round(test_lo, 4)}\n",
    "        \n",
    "    return losses\n",
    "model = GRU(num_embeddings=len(id2lettre), embedding_dim=100, hidden_size=150, output_size=len(id2lettre))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = MaskedCrossEntropy()\n",
    "losses = train(trainloader, model, criterion, optimizer, None, 20)\n",
    "def generate(model, start, maxlength):\n",
    "    x = torch.tensor(string2code(start)).unsqueeze(-1).to(device)\n",
    "    h = [ v.to(device) for v in model.initHidden(1)]\n",
    "    l = [lettre2id[start]]\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(maxlength):\n",
    "            h = model(x, *h)\n",
    "            d = model.decode(h[0])\n",
    "            predictions.append(d.squeeze(0).tolist())\n",
    "            probs = torch.exp(d)\n",
    "            start = torch.distributions.categorical.Categorical(probs).sample()\n",
    "            l.append(start.item())\n",
    "            if start.item() == EOS_IX:\n",
    "                break\n",
    "            start = start.unsqueeze(-1).to(device)\n",
    "    return code2string(l), predictions\n",
    "string, predictions = generate(model, \"x\", 100)\n",
    "def beam_search_decoder(predictions, top_k = 3):\n",
    "    output_sequences = [([], 0)]\n",
    "    for token_probs in predictions:\n",
    "        new_sequences = []\n",
    "    \n",
    "        for old_seq, old_score in output_sequences:\n",
    "            for char_index in range(len(token_probs)):\n",
    "                new_seq = old_seq + [char_index]\n",
    "                new_score = old_score + token_probs[char_index]\n",
    "                new_sequences.append((new_seq, new_score))\n",
    "                \n",
    "        output_sequences = sorted(new_sequences, key = lambda val: val[1], reverse = True)\n",
    "        output_sequences = output_sequences[:top_k]\n",
    "    return output_sequences\n",
    "\n",
    "seqeunces = beam_search_decoder(predictions, top_k = 5)\n",
    "code2string(seqeunces[0][0])\n",
    "\n",
    "import datetime\n",
    "torch.save(model.state_dict(), \"./models/gru-model.pth\")\n",
    "\n",
    "model = GRU(*args, **kwargs)\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing\n",
    "Put here the core of the notebook. Feel free di further split this section into subsections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "We report here relevant references:\n",
    "1. author1, article1, journal1, year1, url1\n",
    "2. author2, article2, journal2, year2, url2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit",
   "language": "python",
   "name": "python37764bita4774d6f44b14c908c5f5fa0bfb95dd2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
